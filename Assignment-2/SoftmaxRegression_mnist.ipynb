{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "mnist = input_data.read_data_sets(\"data\", one_hot = True) # labels are \"one-hot vectors\"\n",
    "\n",
    "# model parameters\n",
    "W = tf.Variable(tf.random_uniform([784, 10])) # 784个像素，10个类\n",
    "b = tf.Variable(tf.random_uniform([10]))\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f970668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist.train.num_examples\n",
    "img = mnist.train.images[0]\n",
    "label = mnist.train.labels[0]\n",
    "#print(img)\n",
    "print(label)\n",
    "\n",
    "# label is a one hot vector 十个数字十个维度，第几个维度是1代表了这个图片的数字标签\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(img.reshape([28,28]), cmap='grey')\n",
    "plt.imshow(img.reshape([28,28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inputs & outputs\n",
    "# placeholder 用来装数据， variable 是变量 会变的\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # \"None\" means that a dimension can be of any length，现在还不确定所以输None\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# creat model\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-70360a36494e>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "# optimizer\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function 预测值和真实值的差距\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "# optimizer\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy test\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) # this gives us a list of booleans\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # booleans->float32\n",
    "\n",
    "# tf.argmax(y,1) 找到每行最大值的坐标， 若第二个数字为0 则是找到每列的最大值坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter step 0, loss 3.844407, training accuracy 0.093750, test accuracy 0.131800\n",
      "iter step 10, loss 1.002800, training accuracy 0.671875, test accuracy 0.636400\n",
      "iter step 20, loss 0.827626, training accuracy 0.734375, test accuracy 0.742900\n",
      "iter step 30, loss 0.265538, training accuracy 0.937500, test accuracy 0.823500\n",
      "iter step 40, loss 0.583942, training accuracy 0.765625, test accuracy 0.828500\n",
      "iter step 50, loss 0.563686, training accuracy 0.859375, test accuracy 0.846000\n",
      "iter step 60, loss 0.781899, training accuracy 0.843750, test accuracy 0.835800\n",
      "iter step 70, loss 0.350690, training accuracy 0.906250, test accuracy 0.857200\n",
      "iter step 80, loss 0.346394, training accuracy 0.906250, test accuracy 0.867200\n",
      "iter step 90, loss 0.500431, training accuracy 0.875000, test accuracy 0.873700\n",
      "iter step 100, loss 0.489026, training accuracy 0.890625, test accuracy 0.870400\n",
      "iter step 110, loss 0.447087, training accuracy 0.906250, test accuracy 0.880200\n",
      "iter step 120, loss 0.353335, training accuracy 0.875000, test accuracy 0.883500\n",
      "iter step 130, loss 0.281968, training accuracy 0.906250, test accuracy 0.880100\n",
      "iter step 140, loss 0.355633, training accuracy 0.937500, test accuracy 0.888900\n",
      "iter step 150, loss 0.726561, training accuracy 0.781250, test accuracy 0.877800\n",
      "iter step 160, loss 0.531663, training accuracy 0.859375, test accuracy 0.883100\n",
      "iter step 170, loss 0.289996, training accuracy 0.906250, test accuracy 0.884900\n",
      "iter step 180, loss 0.408808, training accuracy 0.906250, test accuracy 0.890400\n",
      "iter step 190, loss 0.687190, training accuracy 0.875000, test accuracy 0.883600\n",
      "iter step 200, loss 0.531186, training accuracy 0.859375, test accuracy 0.880800\n",
      "iter step 210, loss 0.547577, training accuracy 0.859375, test accuracy 0.869000\n",
      "iter step 220, loss 0.382169, training accuracy 0.875000, test accuracy 0.890900\n",
      "iter step 230, loss 0.388178, training accuracy 0.890625, test accuracy 0.890400\n",
      "iter step 240, loss 0.213851, training accuracy 0.937500, test accuracy 0.893700\n",
      "iter step 250, loss 0.453026, training accuracy 0.875000, test accuracy 0.897300\n",
      "iter step 260, loss 0.191368, training accuracy 0.921875, test accuracy 0.886400\n",
      "iter step 270, loss 0.342130, training accuracy 0.890625, test accuracy 0.892000\n",
      "iter step 280, loss 0.624871, training accuracy 0.859375, test accuracy 0.895500\n",
      "iter step 290, loss 0.721637, training accuracy 0.859375, test accuracy 0.884700\n",
      "iter step 300, loss 0.538754, training accuracy 0.859375, test accuracy 0.891000\n",
      "iter step 310, loss 0.337176, training accuracy 0.890625, test accuracy 0.893600\n",
      "iter step 320, loss 0.400682, training accuracy 0.921875, test accuracy 0.891700\n",
      "iter step 330, loss 0.357454, training accuracy 0.859375, test accuracy 0.900900\n",
      "iter step 340, loss 0.311084, training accuracy 0.906250, test accuracy 0.897700\n",
      "iter step 350, loss 0.408254, training accuracy 0.875000, test accuracy 0.897100\n",
      "iter step 360, loss 0.364553, training accuracy 0.890625, test accuracy 0.891300\n",
      "iter step 370, loss 0.357701, training accuracy 0.906250, test accuracy 0.903200\n",
      "iter step 380, loss 0.155009, training accuracy 0.953125, test accuracy 0.895600\n",
      "iter step 390, loss 0.493346, training accuracy 0.921875, test accuracy 0.884800\n",
      "iter step 400, loss 0.526450, training accuracy 0.812500, test accuracy 0.896100\n",
      "iter step 410, loss 0.292308, training accuracy 0.875000, test accuracy 0.891400\n",
      "iter step 420, loss 0.177384, training accuracy 0.953125, test accuracy 0.897100\n",
      "iter step 430, loss 0.356508, training accuracy 0.906250, test accuracy 0.901700\n",
      "iter step 440, loss 0.270988, training accuracy 0.890625, test accuracy 0.903800\n",
      "iter step 450, loss 0.549482, training accuracy 0.796875, test accuracy 0.907400\n",
      "iter step 460, loss 0.390195, training accuracy 0.859375, test accuracy 0.905700\n",
      "iter step 470, loss 0.282268, training accuracy 0.890625, test accuracy 0.894100\n",
      "iter step 480, loss 0.312796, training accuracy 0.906250, test accuracy 0.892300\n",
      "iter step 490, loss 0.171200, training accuracy 0.953125, test accuracy 0.903600\n",
      "iter step 500, loss 0.225350, training accuracy 0.906250, test accuracy 0.900200\n",
      "iter step 510, loss 0.533834, training accuracy 0.906250, test accuracy 0.904200\n",
      "iter step 520, loss 0.511329, training accuracy 0.843750, test accuracy 0.902500\n",
      "iter step 530, loss 0.163784, training accuracy 0.968750, test accuracy 0.903400\n",
      "iter step 540, loss 0.287617, training accuracy 0.890625, test accuracy 0.902200\n",
      "iter step 550, loss 0.358879, training accuracy 0.875000, test accuracy 0.898000\n",
      "iter step 560, loss 0.270557, training accuracy 0.937500, test accuracy 0.906800\n",
      "iter step 570, loss 0.523530, training accuracy 0.828125, test accuracy 0.903300\n",
      "iter step 580, loss 0.518694, training accuracy 0.843750, test accuracy 0.902300\n",
      "iter step 590, loss 0.324375, training accuracy 0.921875, test accuracy 0.907200\n",
      "iter step 600, loss 0.683211, training accuracy 0.906250, test accuracy 0.900600\n",
      "iter step 610, loss 0.348968, training accuracy 0.890625, test accuracy 0.910100\n",
      "iter step 620, loss 0.270553, training accuracy 0.937500, test accuracy 0.906700\n",
      "iter step 630, loss 0.478746, training accuracy 0.875000, test accuracy 0.909200\n",
      "iter step 640, loss 0.450711, training accuracy 0.875000, test accuracy 0.900700\n",
      "iter step 650, loss 0.235669, training accuracy 0.937500, test accuracy 0.908800\n",
      "iter step 660, loss 0.360045, training accuracy 0.890625, test accuracy 0.900000\n",
      "iter step 670, loss 0.448253, training accuracy 0.921875, test accuracy 0.904100\n",
      "iter step 680, loss 0.678093, training accuracy 0.812500, test accuracy 0.903200\n",
      "iter step 690, loss 0.518667, training accuracy 0.890625, test accuracy 0.901500\n",
      "iter step 700, loss 0.217875, training accuracy 0.937500, test accuracy 0.905300\n",
      "iter step 710, loss 0.482963, training accuracy 0.890625, test accuracy 0.907600\n",
      "iter step 720, loss 0.293461, training accuracy 0.968750, test accuracy 0.905400\n",
      "iter step 730, loss 0.365308, training accuracy 0.921875, test accuracy 0.896800\n",
      "iter step 740, loss 0.461348, training accuracy 0.875000, test accuracy 0.903300\n",
      "iter step 750, loss 0.337157, training accuracy 0.921875, test accuracy 0.909300\n",
      "iter step 760, loss 0.214091, training accuracy 0.921875, test accuracy 0.908000\n",
      "iter step 770, loss 0.702607, training accuracy 0.859375, test accuracy 0.909200\n",
      "iter step 780, loss 0.235992, training accuracy 0.937500, test accuracy 0.909500\n",
      "iter step 790, loss 0.178465, training accuracy 0.953125, test accuracy 0.897500\n",
      "iter step 800, loss 0.416157, training accuracy 0.890625, test accuracy 0.908900\n",
      "iter step 810, loss 0.405954, training accuracy 0.921875, test accuracy 0.901700\n",
      "iter step 820, loss 0.339970, training accuracy 0.921875, test accuracy 0.909700\n",
      "iter step 830, loss 0.528850, training accuracy 0.906250, test accuracy 0.904100\n",
      "iter step 840, loss 0.393339, training accuracy 0.906250, test accuracy 0.904500\n",
      "iter step 850, loss 0.334623, training accuracy 0.875000, test accuracy 0.904800\n",
      "iter step 860, loss 0.420421, training accuracy 0.921875, test accuracy 0.905800\n",
      "iter step 870, loss 0.334644, training accuracy 0.890625, test accuracy 0.904800\n",
      "iter step 880, loss 0.351537, training accuracy 0.953125, test accuracy 0.912300\n",
      "iter step 890, loss 0.350139, training accuracy 0.890625, test accuracy 0.908200\n",
      "iter step 900, loss 0.379502, training accuracy 0.875000, test accuracy 0.908300\n",
      "iter step 910, loss 0.069760, training accuracy 0.984375, test accuracy 0.910700\n",
      "iter step 920, loss 0.420763, training accuracy 0.890625, test accuracy 0.905600\n",
      "iter step 930, loss 0.251266, training accuracy 0.921875, test accuracy 0.899200\n",
      "iter step 940, loss 0.450228, training accuracy 0.875000, test accuracy 0.900200\n",
      "iter step 950, loss 0.414563, training accuracy 0.906250, test accuracy 0.906500\n",
      "iter step 960, loss 0.246755, training accuracy 0.890625, test accuracy 0.908600\n",
      "iter step 970, loss 0.252081, training accuracy 0.937500, test accuracy 0.915500\n",
      "iter step 980, loss 0.162449, training accuracy 0.937500, test accuracy 0.910300\n",
      "iter step 990, loss 0.284847, training accuracy 0.937500, test accuracy 0.911900\n",
      "iter step 1000, loss 0.084565, training accuracy 0.968750, test accuracy 0.911400\n",
      "iter step 1010, loss 0.717790, training accuracy 0.843750, test accuracy 0.915200\n",
      "iter step 1020, loss 0.146660, training accuracy 0.953125, test accuracy 0.911000\n",
      "iter step 1030, loss 0.323418, training accuracy 0.921875, test accuracy 0.908900\n",
      "iter step 1040, loss 0.147084, training accuracy 0.937500, test accuracy 0.905500\n",
      "iter step 1050, loss 0.184236, training accuracy 0.968750, test accuracy 0.913800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter step 1060, loss 0.220294, training accuracy 0.906250, test accuracy 0.912900\n",
      "iter step 1070, loss 0.210967, training accuracy 0.921875, test accuracy 0.911600\n",
      "iter step 1080, loss 0.372576, training accuracy 0.921875, test accuracy 0.912400\n",
      "iter step 1090, loss 0.215146, training accuracy 0.921875, test accuracy 0.907300\n",
      "iter step 1100, loss 0.316111, training accuracy 0.906250, test accuracy 0.908200\n",
      "iter step 1110, loss 0.253818, training accuracy 0.953125, test accuracy 0.914000\n",
      "iter step 1120, loss 0.266825, training accuracy 0.937500, test accuracy 0.910400\n",
      "iter step 1130, loss 0.280947, training accuracy 0.875000, test accuracy 0.901000\n",
      "iter step 1140, loss 0.299440, training accuracy 0.921875, test accuracy 0.900800\n",
      "iter step 1150, loss 0.289116, training accuracy 0.890625, test accuracy 0.903300\n",
      "iter step 1160, loss 0.533843, training accuracy 0.875000, test accuracy 0.900300\n",
      "iter step 1170, loss 0.279115, training accuracy 0.906250, test accuracy 0.914500\n",
      "iter step 1180, loss 0.407729, training accuracy 0.875000, test accuracy 0.902100\n",
      "iter step 1190, loss 0.204412, training accuracy 0.921875, test accuracy 0.908000\n",
      "iter step 1200, loss 0.300009, training accuracy 0.921875, test accuracy 0.906300\n",
      "iter step 1210, loss 0.342530, training accuracy 0.875000, test accuracy 0.912300\n",
      "iter step 1220, loss 0.285702, training accuracy 0.906250, test accuracy 0.908100\n",
      "iter step 1230, loss 0.310555, training accuracy 0.921875, test accuracy 0.908600\n",
      "iter step 1240, loss 0.165569, training accuracy 0.953125, test accuracy 0.908500\n",
      "iter step 1250, loss 0.436403, training accuracy 0.890625, test accuracy 0.908800\n",
      "iter step 1260, loss 0.493238, training accuracy 0.859375, test accuracy 0.912700\n",
      "iter step 1270, loss 0.364687, training accuracy 0.906250, test accuracy 0.913300\n",
      "iter step 1280, loss 0.394870, training accuracy 0.921875, test accuracy 0.908500\n",
      "iter step 1290, loss 0.291396, training accuracy 0.921875, test accuracy 0.911800\n",
      "iter step 1300, loss 0.110249, training accuracy 0.953125, test accuracy 0.913400\n",
      "iter step 1310, loss 0.337133, training accuracy 0.921875, test accuracy 0.908700\n",
      "iter step 1320, loss 0.214033, training accuracy 0.953125, test accuracy 0.912700\n",
      "iter step 1330, loss 0.344408, training accuracy 0.906250, test accuracy 0.915700\n",
      "iter step 1340, loss 0.265104, training accuracy 0.906250, test accuracy 0.917100\n",
      "iter step 1350, loss 0.251566, training accuracy 0.921875, test accuracy 0.918800\n",
      "iter step 1360, loss 0.170437, training accuracy 0.921875, test accuracy 0.912100\n",
      "iter step 1370, loss 0.405293, training accuracy 0.906250, test accuracy 0.909300\n",
      "iter step 1380, loss 0.345454, training accuracy 0.921875, test accuracy 0.912300\n",
      "iter step 1390, loss 0.325826, training accuracy 0.890625, test accuracy 0.911600\n",
      "iter step 1400, loss 0.314525, training accuracy 0.875000, test accuracy 0.908800\n",
      "iter step 1410, loss 0.338310, training accuracy 0.921875, test accuracy 0.908200\n",
      "iter step 1420, loss 0.276818, training accuracy 0.921875, test accuracy 0.910800\n",
      "iter step 1430, loss 0.104532, training accuracy 0.953125, test accuracy 0.905500\n",
      "iter step 1440, loss 0.440200, training accuracy 0.890625, test accuracy 0.917100\n",
      "iter step 1450, loss 0.200537, training accuracy 0.968750, test accuracy 0.909900\n",
      "iter step 1460, loss 0.494205, training accuracy 0.875000, test accuracy 0.914200\n",
      "iter step 1470, loss 0.181827, training accuracy 0.921875, test accuracy 0.913200\n",
      "iter step 1480, loss 0.300574, training accuracy 0.953125, test accuracy 0.907100\n",
      "iter step 1490, loss 0.152126, training accuracy 0.937500, test accuracy 0.908600\n",
      "iter step 1500, loss 0.246404, training accuracy 0.906250, test accuracy 0.910900\n",
      "iter step 1510, loss 0.125822, training accuracy 0.968750, test accuracy 0.909500\n",
      "iter step 1520, loss 0.605823, training accuracy 0.828125, test accuracy 0.905200\n",
      "iter step 1530, loss 0.236313, training accuracy 0.937500, test accuracy 0.910400\n",
      "iter step 1540, loss 0.274586, training accuracy 0.937500, test accuracy 0.913300\n",
      "iter step 1550, loss 0.366065, training accuracy 0.921875, test accuracy 0.905200\n",
      "iter step 1560, loss 0.271698, training accuracy 0.937500, test accuracy 0.909300\n",
      "iter step 1570, loss 0.299945, training accuracy 0.890625, test accuracy 0.907900\n",
      "iter step 1580, loss 0.216269, training accuracy 0.921875, test accuracy 0.904800\n",
      "iter step 1590, loss 0.372475, training accuracy 0.890625, test accuracy 0.908500\n",
      "iter step 1600, loss 0.340847, training accuracy 0.921875, test accuracy 0.898300\n",
      "iter step 1610, loss 0.217273, training accuracy 0.937500, test accuracy 0.917700\n",
      "iter step 1620, loss 0.120699, training accuracy 0.968750, test accuracy 0.914200\n",
      "iter step 1630, loss 0.478393, training accuracy 0.875000, test accuracy 0.915200\n",
      "iter step 1640, loss 0.204869, training accuracy 0.921875, test accuracy 0.912800\n",
      "iter step 1650, loss 0.342221, training accuracy 0.875000, test accuracy 0.906100\n",
      "iter step 1660, loss 0.406420, training accuracy 0.875000, test accuracy 0.914000\n",
      "iter step 1670, loss 0.348808, training accuracy 0.890625, test accuracy 0.912200\n",
      "iter step 1680, loss 0.337804, training accuracy 0.890625, test accuracy 0.909700\n",
      "iter step 1690, loss 0.390318, training accuracy 0.890625, test accuracy 0.905500\n",
      "iter step 1700, loss 0.176161, training accuracy 0.921875, test accuracy 0.908300\n",
      "iter step 1710, loss 0.271058, training accuracy 0.953125, test accuracy 0.911900\n",
      "iter step 1720, loss 0.510943, training accuracy 0.875000, test accuracy 0.911200\n",
      "iter step 1730, loss 0.153861, training accuracy 0.937500, test accuracy 0.905100\n",
      "iter step 1740, loss 0.106111, training accuracy 0.984375, test accuracy 0.908400\n",
      "iter step 1750, loss 0.396469, training accuracy 0.859375, test accuracy 0.906600\n",
      "iter step 1760, loss 0.279119, training accuracy 0.937500, test accuracy 0.907600\n",
      "iter step 1770, loss 0.489287, training accuracy 0.906250, test accuracy 0.905300\n",
      "iter step 1780, loss 0.253260, training accuracy 0.921875, test accuracy 0.911500\n",
      "iter step 1790, loss 0.275001, training accuracy 0.906250, test accuracy 0.917200\n",
      "iter step 1800, loss 0.263571, training accuracy 0.921875, test accuracy 0.914000\n",
      "iter step 1810, loss 0.371955, training accuracy 0.906250, test accuracy 0.911300\n",
      "iter step 1820, loss 0.338099, training accuracy 0.875000, test accuracy 0.913000\n",
      "iter step 1830, loss 0.210731, training accuracy 0.953125, test accuracy 0.913100\n",
      "iter step 1840, loss 0.114376, training accuracy 0.984375, test accuracy 0.907000\n",
      "iter step 1850, loss 0.230365, training accuracy 0.906250, test accuracy 0.914200\n",
      "iter step 1860, loss 0.227249, training accuracy 0.921875, test accuracy 0.910600\n",
      "iter step 1870, loss 0.429317, training accuracy 0.906250, test accuracy 0.915300\n",
      "iter step 1880, loss 0.279203, training accuracy 0.875000, test accuracy 0.915800\n",
      "iter step 1890, loss 0.258946, training accuracy 0.906250, test accuracy 0.914100\n",
      "iter step 1900, loss 0.113190, training accuracy 0.984375, test accuracy 0.907100\n",
      "iter step 1910, loss 0.407453, training accuracy 0.906250, test accuracy 0.901100\n",
      "iter step 1920, loss 0.377112, training accuracy 0.937500, test accuracy 0.914000\n",
      "iter step 1930, loss 0.345240, training accuracy 0.875000, test accuracy 0.905200\n",
      "iter step 1940, loss 0.366529, training accuracy 0.921875, test accuracy 0.915400\n",
      "iter step 1950, loss 0.528656, training accuracy 0.890625, test accuracy 0.913600\n",
      "iter step 1960, loss 0.399040, training accuracy 0.906250, test accuracy 0.908000\n",
      "iter step 1970, loss 0.491068, training accuracy 0.906250, test accuracy 0.917700\n",
      "iter step 1980, loss 0.429405, training accuracy 0.859375, test accuracy 0.917700\n",
      "iter step 1990, loss 0.120093, training accuracy 0.968750, test accuracy 0.918300\n",
      "iter step 2000, loss 0.444189, training accuracy 0.843750, test accuracy 0.913200\n",
      "iter step 2010, loss 0.242795, training accuracy 0.906250, test accuracy 0.909200\n",
      "iter step 2020, loss 0.088471, training accuracy 0.968750, test accuracy 0.918500\n",
      "iter step 2030, loss 0.140293, training accuracy 0.953125, test accuracy 0.915800\n",
      "iter step 2040, loss 0.263425, training accuracy 0.890625, test accuracy 0.917900\n",
      "iter step 2050, loss 0.392331, training accuracy 0.890625, test accuracy 0.916500\n",
      "iter step 2060, loss 0.231891, training accuracy 0.953125, test accuracy 0.915200\n",
      "iter step 2070, loss 0.167863, training accuracy 0.953125, test accuracy 0.910200\n",
      "iter step 2080, loss 0.180635, training accuracy 0.921875, test accuracy 0.915900\n",
      "iter step 2090, loss 0.269195, training accuracy 0.906250, test accuracy 0.917500\n",
      "iter step 2100, loss 0.282440, training accuracy 0.828125, test accuracy 0.906000\n",
      "iter step 2110, loss 0.439473, training accuracy 0.921875, test accuracy 0.915800\n",
      "iter step 2120, loss 0.315993, training accuracy 0.953125, test accuracy 0.910100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter step 2130, loss 0.223723, training accuracy 0.937500, test accuracy 0.904500\n",
      "iter step 2140, loss 0.469911, training accuracy 0.890625, test accuracy 0.908000\n",
      "iter step 2150, loss 0.276297, training accuracy 0.921875, test accuracy 0.910100\n",
      "iter step 2160, loss 0.286593, training accuracy 0.921875, test accuracy 0.908600\n",
      "iter step 2170, loss 0.321475, training accuracy 0.875000, test accuracy 0.912300\n",
      "iter step 2180, loss 0.153523, training accuracy 0.953125, test accuracy 0.909300\n",
      "iter step 2190, loss 0.220897, training accuracy 0.921875, test accuracy 0.919800\n",
      "iter step 2200, loss 0.319350, training accuracy 0.906250, test accuracy 0.919200\n",
      "iter step 2210, loss 0.375083, training accuracy 0.906250, test accuracy 0.911300\n",
      "iter step 2220, loss 0.257954, training accuracy 0.921875, test accuracy 0.914000\n",
      "iter step 2230, loss 0.360764, training accuracy 0.859375, test accuracy 0.914400\n",
      "iter step 2240, loss 0.255941, training accuracy 0.921875, test accuracy 0.912900\n",
      "iter step 2250, loss 0.232687, training accuracy 0.921875, test accuracy 0.911700\n",
      "iter step 2260, loss 0.439244, training accuracy 0.890625, test accuracy 0.913400\n",
      "iter step 2270, loss 0.364263, training accuracy 0.875000, test accuracy 0.909100\n",
      "iter step 2280, loss 0.298812, training accuracy 0.906250, test accuracy 0.916900\n",
      "iter step 2290, loss 0.395724, training accuracy 0.875000, test accuracy 0.911200\n",
      "iter step 2300, loss 0.480985, training accuracy 0.890625, test accuracy 0.916900\n",
      "iter step 2310, loss 0.181466, training accuracy 0.968750, test accuracy 0.915300\n",
      "iter step 2320, loss 0.556287, training accuracy 0.843750, test accuracy 0.911300\n",
      "iter step 2330, loss 0.235279, training accuracy 0.937500, test accuracy 0.914100\n",
      "iter step 2340, loss 0.251675, training accuracy 0.906250, test accuracy 0.911800\n",
      "iter step 2350, loss 0.436659, training accuracy 0.859375, test accuracy 0.913500\n",
      "iter step 2360, loss 0.334249, training accuracy 0.875000, test accuracy 0.911400\n",
      "iter step 2370, loss 0.434694, training accuracy 0.921875, test accuracy 0.917800\n",
      "iter step 2380, loss 0.361332, training accuracy 0.890625, test accuracy 0.911100\n",
      "iter step 2390, loss 0.111140, training accuracy 0.968750, test accuracy 0.904000\n",
      "iter step 2400, loss 0.106104, training accuracy 0.953125, test accuracy 0.917200\n",
      "iter step 2410, loss 0.101504, training accuracy 0.953125, test accuracy 0.919400\n",
      "iter step 2420, loss 0.255663, training accuracy 0.906250, test accuracy 0.905600\n",
      "iter step 2430, loss 0.373151, training accuracy 0.921875, test accuracy 0.915900\n",
      "iter step 2440, loss 0.322410, training accuracy 0.890625, test accuracy 0.920400\n",
      "iter step 2450, loss 0.426445, training accuracy 0.921875, test accuracy 0.914000\n",
      "iter step 2460, loss 0.292512, training accuracy 0.921875, test accuracy 0.917900\n",
      "iter step 2470, loss 0.310780, training accuracy 0.906250, test accuracy 0.917100\n",
      "iter step 2480, loss 0.326922, training accuracy 0.906250, test accuracy 0.917100\n",
      "iter step 2490, loss 0.243443, training accuracy 0.890625, test accuracy 0.910000\n",
      "iter step 2500, loss 0.407638, training accuracy 0.875000, test accuracy 0.910100\n",
      "iter step 2510, loss 0.085918, training accuracy 0.968750, test accuracy 0.917500\n",
      "iter step 2520, loss 0.454289, training accuracy 0.921875, test accuracy 0.913800\n",
      "iter step 2530, loss 0.272186, training accuracy 0.921875, test accuracy 0.917700\n",
      "iter step 2540, loss 0.460066, training accuracy 0.890625, test accuracy 0.914100\n",
      "iter step 2550, loss 0.440907, training accuracy 0.828125, test accuracy 0.920500\n",
      "iter step 2560, loss 0.394521, training accuracy 0.921875, test accuracy 0.917900\n",
      "iter step 2570, loss 0.350621, training accuracy 0.890625, test accuracy 0.915600\n",
      "iter step 2580, loss 0.398154, training accuracy 0.828125, test accuracy 0.911500\n",
      "iter step 2590, loss 0.213523, training accuracy 0.921875, test accuracy 0.921400\n",
      "iter step 2600, loss 0.195801, training accuracy 0.906250, test accuracy 0.909300\n",
      "iter step 2610, loss 0.289222, training accuracy 0.937500, test accuracy 0.916600\n",
      "iter step 2620, loss 0.166566, training accuracy 0.953125, test accuracy 0.903900\n",
      "iter step 2630, loss 0.086451, training accuracy 0.984375, test accuracy 0.911600\n",
      "iter step 2640, loss 0.218079, training accuracy 0.937500, test accuracy 0.910700\n",
      "iter step 2650, loss 0.270016, training accuracy 0.906250, test accuracy 0.914500\n",
      "iter step 2660, loss 0.229559, training accuracy 0.921875, test accuracy 0.917300\n",
      "iter step 2670, loss 0.214906, training accuracy 0.937500, test accuracy 0.911700\n",
      "iter step 2680, loss 0.295534, training accuracy 0.890625, test accuracy 0.920600\n",
      "iter step 2690, loss 0.130800, training accuracy 0.984375, test accuracy 0.916800\n",
      "iter step 2700, loss 0.173250, training accuracy 0.937500, test accuracy 0.915000\n",
      "iter step 2710, loss 0.710502, training accuracy 0.843750, test accuracy 0.913400\n",
      "iter step 2720, loss 0.189219, training accuracy 0.937500, test accuracy 0.913200\n",
      "iter step 2730, loss 0.426223, training accuracy 0.921875, test accuracy 0.914000\n",
      "iter step 2740, loss 0.364192, training accuracy 0.906250, test accuracy 0.916300\n",
      "iter step 2750, loss 0.302566, training accuracy 0.937500, test accuracy 0.918000\n",
      "iter step 2760, loss 0.259028, training accuracy 0.906250, test accuracy 0.913500\n",
      "iter step 2770, loss 0.163625, training accuracy 0.937500, test accuracy 0.911700\n",
      "iter step 2780, loss 0.270176, training accuracy 0.906250, test accuracy 0.915100\n",
      "iter step 2790, loss 0.469519, training accuracy 0.890625, test accuracy 0.908000\n",
      "iter step 2800, loss 0.370779, training accuracy 0.906250, test accuracy 0.908500\n",
      "iter step 2810, loss 0.186233, training accuracy 0.937500, test accuracy 0.911500\n",
      "iter step 2820, loss 0.442657, training accuracy 0.906250, test accuracy 0.914400\n",
      "iter step 2830, loss 0.309052, training accuracy 0.890625, test accuracy 0.912200\n",
      "iter step 2840, loss 0.436050, training accuracy 0.921875, test accuracy 0.910300\n",
      "iter step 2850, loss 0.397143, training accuracy 0.921875, test accuracy 0.909500\n",
      "iter step 2860, loss 0.696792, training accuracy 0.921875, test accuracy 0.918300\n",
      "iter step 2870, loss 0.310224, training accuracy 0.875000, test accuracy 0.913400\n",
      "iter step 2880, loss 0.187460, training accuracy 0.937500, test accuracy 0.917300\n",
      "iter step 2890, loss 0.375828, training accuracy 0.906250, test accuracy 0.916700\n",
      "iter step 2900, loss 0.239784, training accuracy 0.937500, test accuracy 0.917200\n",
      "iter step 2910, loss 0.225480, training accuracy 0.968750, test accuracy 0.919200\n",
      "iter step 2920, loss 0.355830, training accuracy 0.937500, test accuracy 0.911300\n",
      "iter step 2930, loss 0.283581, training accuracy 0.937500, test accuracy 0.907800\n",
      "iter step 2940, loss 0.550100, training accuracy 0.875000, test accuracy 0.917600\n",
      "iter step 2950, loss 0.161328, training accuracy 0.968750, test accuracy 0.912700\n",
      "iter step 2960, loss 0.337252, training accuracy 0.953125, test accuracy 0.918700\n",
      "iter step 2970, loss 0.090981, training accuracy 0.968750, test accuracy 0.920400\n",
      "iter step 2980, loss 0.431298, training accuracy 0.906250, test accuracy 0.917100\n",
      "iter step 2990, loss 0.293294, training accuracy 0.921875, test accuracy 0.906300\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:  # sess = tf.Session()\n",
    "    sess.run(init) # 初始化\n",
    "    for iter in range(3000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        test_x = mnist.test.images\n",
    "        test_y = mnist.test.labels\n",
    "        sess.run(train_step, {x:batch_x, y_:batch_y})\n",
    "        if iter%10 == 0:\n",
    "            train_loss = sess.run(cross_entropy,{x:batch_x,y_:batch_y})\n",
    "            train_accuracy = sess.run(accuracy, {x:batch_x,y_:batch_y})\n",
    "            test_accuracy = sess.run(accuracy, {x: test_x, y_: test_y})\n",
    "            print(\"iter step %d, loss %f, training accuracy %f, test accuracy %f\" %\n",
    "                (iter,train_loss,train_accuracy,test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
